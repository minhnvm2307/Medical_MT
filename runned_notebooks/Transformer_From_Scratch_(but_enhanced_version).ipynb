{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2cf14a2fb3cf4dbfa8a607bc6e01a008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3086dc738844542bf5738e58a7ea282"
            ],
            "layout": "IPY_MODEL_f4c1f5e3e02b4c3d86706d99f6508360"
          }
        },
        "bd04944711574749b5f00ba17814aadb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a481e1798824c6885e7544feafcf7b8",
            "placeholder": "​",
            "style": "IPY_MODEL_73a9f8f46bd54be7b672f0362bab548c",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "c74dd7d0da0a4ac68757a9fcc2ae684f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_05149e4f0efc44b29748f41f89a2c6eb",
            "placeholder": "​",
            "style": "IPY_MODEL_3429903bec0346df9d1b704d1a3499f0",
            "value": "kwahnguyen"
          }
        },
        "4b0a1d4a07434055b76b9d7a7c1a4b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c130b22940424578bf15e6829c7f5c73",
            "placeholder": "​",
            "style": "IPY_MODEL_96deae39f03149e7806cd3e68a021cda",
            "value": ""
          }
        },
        "86382778f00e480e9221ba8652ce2dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f0554e0e5b7a404f9a575038f68e4276",
            "style": "IPY_MODEL_1c71b821985d46149420c1eaef15d8f1",
            "tooltip": ""
          }
        },
        "9dec1dea263a4d88bdb1402af6d51c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32f10cf0893e445285a15a07aa27baf5",
            "placeholder": "​",
            "style": "IPY_MODEL_07f3d37c26a24e1195f74db8c0643478",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "f4c1f5e3e02b4c3d86706d99f6508360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "0a481e1798824c6885e7544feafcf7b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a9f8f46bd54be7b672f0362bab548c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05149e4f0efc44b29748f41f89a2c6eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3429903bec0346df9d1b704d1a3499f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c130b22940424578bf15e6829c7f5c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96deae39f03149e7806cd3e68a021cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0554e0e5b7a404f9a575038f68e4276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c71b821985d46149420c1eaef15d8f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "32f10cf0893e445285a15a07aa27baf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f3d37c26a24e1195f74db8c0643478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a0e95bcdb20412f922b0bbff3e23d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_055e1d1b581c4daf9a705c3199c2babc",
            "placeholder": "​",
            "style": "IPY_MODEL_94f2aca14bde4d0f9602da54ad6bb1d7",
            "value": "Connecting..."
          }
        },
        "055e1d1b581c4daf9a705c3199c2babc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f2aca14bde4d0f9602da54ad6bb1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6b6238007074ec2873ba82529840a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_978b1b2005004203adc03fb82ee04ad2",
            "placeholder": "​",
            "style": "IPY_MODEL_537dbc242d1b4a22b0b1af111722211a",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "978b1b2005004203adc03fb82ee04ad2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537dbc242d1b4a22b0b1af111722211a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25068381ab0c4e0585bdf7b0e44302fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f5b8ed73d0b4f14ba59d2431816c259",
            "placeholder": "​",
            "style": "IPY_MODEL_de01de5ca0804811b1e54517e211a5f5",
            "value": "Connecting..."
          }
        },
        "7f5b8ed73d0b4f14ba59d2431816c259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de01de5ca0804811b1e54517e211a5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3086dc738844542bf5738e58a7ea282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e88fb21bd8f54c73a3fa4974e4e1eb90",
            "placeholder": "​",
            "style": "IPY_MODEL_feb8cb76e7834cda9ee8e96f44f16160",
            "value": "Both username and API key cannot be empty or whitespace"
          }
        },
        "e88fb21bd8f54c73a3fa4974e4e1eb90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feb8cb76e7834cda9ee8e96f44f16160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA9YerXu5cP4",
        "outputId": "c3877e3d-bc8c-4d75-97f6-0b87d9a0e68b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting https://gitlab.com/trungtv/vi_spacy/-/raw/master/packages/vi_core_news_lg-3.6.0/dist/vi_core_news_lg-3.6.0.tar.gz\n",
            "  Downloading https://gitlab.com/trungtv/vi_spacy/-/raw/master/packages/vi_core_news_lg-3.6.0/dist/vi_core_news_lg-3.6.0.tar.gz (233.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.3/233.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.7.0,>=3.6.0 (from vi_core_news_lg==3.6.0)\n",
            "  Downloading spacy-3.6.1.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31mDeprecationWarning: The command 'link' is deprecated.\u001b[0m\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, model symlinks are not supported anymore. You can\n",
            "load trained pipeline packages using their full names or from a directory\n",
            "path.\u001b[0m\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "! pip -q install torchtext==0.6.0\n",
        "! pip -q install pyvi\n",
        "! pip install https://gitlab.com/trungtv/vi_spacy/-/raw/master/packages/vi_core_news_lg-3.6.0/dist/vi_core_news_lg-3.6.0.tar.gz\n",
        "! python -m spacy link vi_spacy_model vi_spacy_model\n",
        "!pip install sacrebleu\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc3mzcss-VhT",
        "outputId": "b1a77b91-fe11-400d-b18b-94ee3595cf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import math"
      ],
      "metadata": {
        "id": "xx4cE2io50n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Training helpers**"
      ],
      "metadata": {
        "id": "Qz9x7xm8531w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "# Embedder(100, 512)(torch.LongTensor([1,2,3,4])).shape"
      ],
      "metadata": {
        "id": "Y9EKTEWZ53Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Innovation: RoPE implementation**"
      ],
      "metadata": {
        "id": "Zaz7y0bH6K28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPE(nn.Module):\n",
        "    \"\"\"\n",
        "    A conceptual implementation of Rotary Positional Embedding (RoPE).\n",
        "    It generates and applies the rotation matrix/tensor to Q and K vectors.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "        t = torch.arange(max_len, dtype=torch.float)\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
        "\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "\n",
        "        self.register_buffer(\"cos_cached\", emb.cos()[:, None, None, :], persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin()[:, None, None, :], persistent=False)\n",
        "\n",
        "    def forward(self, x, seq_len):\n",
        "            # x shape: (B, H, N, D_k)\n",
        "            B, H, N, D_k = x.shape\n",
        "\n",
        "            cos = self.cos_cached[:seq_len, :].to(x.device, dtype=x.dtype)\n",
        "            sin = self.sin_cached[:seq_len, :].to(x.device, dtype=x.dtype)\n",
        "\n",
        "            cos = cos.permute(2, 1, 0, 3)\n",
        "            sin = sin.permute(2, 1, 0, 3)\n",
        "\n",
        "            d = self.d_model # D_k\n",
        "\n",
        "            x_rot = x[..., :d//2]\n",
        "            x_pass = x[..., d//2:]\n",
        "\n",
        "            rotated_x = torch.cat((-x_pass, x_rot), dim=-1)\n",
        "\n",
        "            return (x * cos) + (rotated_x * sin)"
      ],
      "metadata": {
        "id": "_bkAOsqS57fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Attention Mechanism**"
      ],
      "metadata": {
        "id": "9P67T-6e6TYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(q, k, v, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    q: batch_size x head x seq_length x d_model\n",
        "    k: batch_size x head x seq_length x d_model\n",
        "    v: batch_size x head x seq_length x d_model\n",
        "    mask: batch_size x 1 x 1 x seq_length\n",
        "    output: batch_size x head x seq_length x d_model\n",
        "    \"\"\"\n",
        "\n",
        "    # attention score được tính bằng cách nhân q với k\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "    # xong rồi thì chuẩn hóa bằng softmax\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "\n",
        "    output = torch.matmul(scores, v)\n",
        "    return output, scores\n",
        "\n",
        "# attention(torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512)).shape"
      ],
      "metadata": {
        "id": "mT9troi06SaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        self.attn = None\n",
        "\n",
        "        # RoPE requires d_model to be a multiple of 2 (which is true for common d_model values)\n",
        "        # For strict RoPE, the rotation is applied only on the d_k dimension.\n",
        "        self.rope = RoPE(d_model=self.d_k, max_len=512) # RoPE instance for head dimension\n",
        "\n",
        "        # Linear projections\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        bs = q.size(0)\n",
        "        # Note: We still calculate seq_len here, but we will not use it in the view.\n",
        "        # seq_len = q.size(1)\n",
        "\n",
        "        # 1. Linear Projection (B, N, D) -> (B, N, D)\n",
        "        q = self.q_linear(q)\n",
        "        k = self.k_linear(k)\n",
        "        v = self.v_linear(v)\n",
        "\n",
        "        # 2. Split into heads and transpose (B, N, D) -> (B, H, N, D_k)\n",
        "        # We use -1 for the sequence length dimension (index 1)\n",
        "        q = q.view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        k = k.view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        v = v.view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # ... rest of the code (including RoPE) ...\n",
        "        # If you need the sequence length later for RoPE (N), you can get it from the\n",
        "        # reshaped tensor: N = q.size(2)\n",
        "\n",
        "        q_len = q.size(2)\n",
        "        k_len = k.size(2)\n",
        "        # 3. APPLY RoPE to Q and K\n",
        "        q = self.rope(q, seq_len=q_len) # Use the dynamically calculated length\n",
        "        k = self.rope(k, seq_len=k_len) # Use the dynamically calculated length\n",
        "\n",
        "        # 4. Compute Attention Scores (Scores, Attn Weights)\n",
        "        scores, self.attn = attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        # 5. Concatenate Heads (B, H, N, D_k) -> (B, N, D)\n",
        "        # scores has shape (B, H, N, D_k)\n",
        "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
        "\n",
        "        # 6. Final Linear Output\n",
        "        output = self.out(concat)\n",
        "        return output"
      ],
      "metadata": {
        "id": "MhgBz_MQ6ZN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.size = d_model\n",
        "\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "metadata": {
        "id": "QoImxDyD6ZwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Innovation: Using SwiGLU as activation unit**"
      ],
      "metadata": {
        "id": "gciwbBpv6dOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Two parallel projections\n",
        "        self.w = nn.Linear(d_model, d_ff)   # gate branch\n",
        "        self.v = nn.Linear(d_model, d_ff)   # linear branch\n",
        "\n",
        "        # Output projection\n",
        "        self.w2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # SwiGLU: silu(xW) ⊗ xV\n",
        "        x = F.silu(self.w(x)) * self.v(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.w2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "t8y_no3S6cr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prelayer Normalization\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Norms are initialized, same as before\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout=dropout)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        norm_x = self.norm_1(x)\n",
        "\n",
        "        attn_out = self.attn(norm_x, norm_x, norm_x, mask)\n",
        "\n",
        "        x = x + self.dropout_1(attn_out)\n",
        "\n",
        "        norm_x = self.norm_2(x)\n",
        "\n",
        "        ffn_out = self.ff(norm_x)\n",
        "\n",
        "        x = x + self.dropout_2(ffn_out)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "lbfoFhrF6kxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Prelayer Norm\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Norms are initialized, same as before\n",
        "        self.norm_1 = Norm(d_model) # For Masked Self-Attention\n",
        "        self.norm_2 = Norm(d_model) # For Encoder-Decoder Attention\n",
        "        self.norm_3 = Norm(d_model) # For Feed-Forward Network\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout) # Masked Self-Attention\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout) # Cross-Attention\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        norm_x = self.norm_1(x)\n",
        "\n",
        "        attn_out = self.attn_1(norm_x, norm_x, norm_x, trg_mask)\n",
        "\n",
        "        x = x + self.dropout_1(attn_out)\n",
        "\n",
        "        norm_x = self.norm_2(x)\n",
        "\n",
        "        attn_out = self.attn_2(norm_x, e_outputs, e_outputs, src_mask)\n",
        "\n",
        "        x = x + self.dropout_2(attn_out)\n",
        "\n",
        "        norm_x = self.norm_3(x)\n",
        "\n",
        "        ffn_out = self.ff(norm_x)\n",
        "\n",
        "        x = x + self.dropout_3(ffn_out)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "a9BxFo3D6sYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        # self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads, d_ff, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(src)\n",
        "        # x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Encoder(232, 512,6,8,0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 1, 30)).shape"
      ],
      "metadata": {
        "id": "oTJcgjBf62a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        # self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads, d_ff, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        trg: batch_size x seq_length\n",
        "        e_outputs: batch_size x seq_length x d_model\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(trg)\n",
        "        # x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Decoder(232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
      ],
      "metadata": {
        "id": "iLOyFJVN641Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, d_ff, dropout)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        trg: batch_size x seq_length\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x vocab_size\n",
        "        \"\"\"\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "\n",
        "# Transformer(232, 232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.LongTensor(32, 30).random_(0, 10),torch.rand(32, 1, 30),torch.rand(32, 1, 30)).shape"
      ],
      "metadata": {
        "id": "WPGLX_ho67QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Preparing the dataset**"
      ],
      "metadata": {
        "id": "PQlpO00U7BEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96,
          "referenced_widgets": [
            "2cf14a2fb3cf4dbfa8a607bc6e01a008",
            "bd04944711574749b5f00ba17814aadb",
            "c74dd7d0da0a4ac68757a9fcc2ae684f",
            "4b0a1d4a07434055b76b9d7a7c1a4b5f",
            "86382778f00e480e9221ba8652ce2dac",
            "9dec1dea263a4d88bdb1402af6d51c64",
            "f4c1f5e3e02b4c3d86706d99f6508360",
            "0a481e1798824c6885e7544feafcf7b8",
            "73a9f8f46bd54be7b672f0362bab548c",
            "05149e4f0efc44b29748f41f89a2c6eb",
            "3429903bec0346df9d1b704d1a3499f0",
            "c130b22940424578bf15e6829c7f5c73",
            "96deae39f03149e7806cd3e68a021cda",
            "f0554e0e5b7a404f9a575038f68e4276",
            "1c71b821985d46149420c1eaef15d8f1",
            "32f10cf0893e445285a15a07aa27baf5",
            "07f3d37c26a24e1195f74db8c0643478",
            "9a0e95bcdb20412f922b0bbff3e23d53",
            "055e1d1b581c4daf9a705c3199c2babc",
            "94f2aca14bde4d0f9602da54ad6bb1d7",
            "c6b6238007074ec2873ba82529840a3e",
            "978b1b2005004203adc03fb82ee04ad2",
            "537dbc242d1b4a22b0b1af111722211a",
            "25068381ab0c4e0585bdf7b0e44302fc",
            "7f5b8ed73d0b4f14ba59d2431816c259",
            "de01de5ca0804811b1e54517e211a5f5",
            "d3086dc738844542bf5738e58a7ea282",
            "e88fb21bd8f54c73a3fa4974e4e1eb90",
            "feb8cb76e7834cda9ee8e96f44f16160"
          ]
        },
        "id": "0Sv6ZQrR90QL",
        "outputId": "927ab44a-a3f0-4cf3-9be6-70314028cdfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cf14a2fb3cf4dbfa8a607bc6e01a008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run to use VLSP dataset\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nguynvitcng21020173/vlsp-2025-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f20g9n067Cvs",
        "outputId": "1a4f4354-2f37-494d-cc4a-90ed993b743f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/nguynvitcng21020173/vlsp-2025-data?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.1M/49.1M [00:00<00:00, 162MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/nguynvitcng21020173/vlsp-2025-data/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /root/.cache/kagglehub/datasets/nguynvitcng21020173/vlsp-2025-data/versions/1 /content/VLSP_data"
      ],
      "metadata": {
        "id": "f2pKxNQ0931Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10000 '/content/VLSP_data/train.en.txt' > '/content/VLSP_data/valid.en'\n",
        "!head -n 10000 '/content/VLSP_data/train.vi.txt' > '/content/VLSP_data/valid.vi'\n",
        "!tail -n +10001 '/content/VLSP_data/train.en.txt' > '/content/VLSP_data/train_new.en'\n",
        "!tail -n +10001 '/content/VLSP_data/train.vi.txt' > '/content/VLSP_data/train_new.vi'"
      ],
      "metadata": {
        "id": "iI3A-yR-95j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Innovation: Using BPE as tokenizer**"
      ],
      "metadata": {
        "id": "otVk74Ak7OYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import os\n",
        "\n",
        "# Initialize\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Train on both source and target training files\n",
        "tokenizer.train(\n",
        "    files=[\"/content/VLSP_data/train_new.en\", \"/content/VLSP_data/train_new.vi\"],  # both languages\n",
        "    vocab_size=30000,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n",
        ")\n",
        "\n",
        "os.makedirs(\"bpe_tokenizer\", exist_ok=True)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_model(\"bpe_tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15UAe_Cq7FE2",
        "outputId": "3908a8b1-dcd8-4b3b-f904-c4db0ae324f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bpe_tokenizer/vocab.json', 'bpe_tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "bpe_tokenizer = ByteLevelBPETokenizer(\n",
        "    \"bpe_tokenizer/vocab.json\",\n",
        "    \"bpe_tokenizer/merges.txt\"\n",
        ")\n",
        "\n",
        "PAD_ID = bpe_tokenizer.token_to_id(\"<pad>\")\n",
        "SOS_ID = bpe_tokenizer.token_to_id(\"<s>\")\n",
        "EOS_ID = bpe_tokenizer.token_to_id(\"</s>\")"
      ],
      "metadata": {
        "id": "XHnfozYY7NMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiYbB1F57T95",
        "outputId": "a3e8f29a-018a-4fd3-cbd6-eb0f492d2828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext import data\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "\n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "metadata": {
        "id": "NYE8tIQg7Woe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nopeak_mask(size, device):\n",
        "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
        "     mô hình không nhìn thấy được các từ ở tương lai\n",
        "    \"\"\"\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\n",
        "    k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    np_mask = np_mask.to(device)\n",
        "\n",
        "    return np_mask\n",
        "\n",
        "def create_masks(src, trg, src_pad, trg_pad, device):\n",
        "    \"\"\" Tạo mask cho encoder,\n",
        "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào\n",
        "    \"\"\"\n",
        "    src_mask = (src != src_pad).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size, device)\n",
        "        if trg.is_cuda:\n",
        "            np_mask.cuda()\n",
        "        trg_mask = trg_mask & np_mask\n",
        "\n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ],
      "metadata": {
        "id": "km29iAh67YfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_vars(src, model, device, k, max_len):\n",
        "    \"\"\"\n",
        "    Initialize variables for beam search (BPE version)\n",
        "    \"\"\"\n",
        "\n",
        "    # Special token IDs (global or imported)\n",
        "    init_tok = SOS_ID\n",
        "\n",
        "    # Source padding mask\n",
        "    src_mask = (src != PAD_ID).unsqueeze(-2)\n",
        "\n",
        "    # Encoder output\n",
        "    e_output = model.encoder(src, src_mask)\n",
        "\n",
        "    # Initialize decoder input with <s>\n",
        "    outputs = torch.LongTensor([[init_tok]]).to(device)\n",
        "\n",
        "    # First decoding step\n",
        "    trg_mask = nopeak_mask(1, device)\n",
        "    out = model.out(\n",
        "        model.decoder(outputs, e_output, src_mask, trg_mask)\n",
        "    )\n",
        "    out = F.softmax(out, dim=-1)\n",
        "\n",
        "    # Top-k candidates\n",
        "    probs, ix = out[:, -1].topk(k)\n",
        "    log_scores = torch.log(probs)\n",
        "\n",
        "    # Prepare beam outputs\n",
        "    outputs = torch.zeros(k, max_len, dtype=torch.long).to(device)\n",
        "    outputs[:, 0] = init_tok\n",
        "    outputs[:, 1] = ix[0]\n",
        "\n",
        "    # Repeat encoder outputs for k beams\n",
        "    e_outputs = e_output.repeat(k, 1, 1)\n",
        "\n",
        "    return outputs, e_outputs, log_scores\n",
        "\n",
        "def k_best_outputs(outputs, out, log_scores, i, k):\n",
        "    \"\"\"\n",
        "    Select k best next tokens for beam search\n",
        "    \"\"\"\n",
        "\n",
        "    # out: [k, i, vocab_size]\n",
        "    probs, ix = out[:, -1].topk(k)          # [k, k]\n",
        "    log_probs = torch.log(probs) + log_scores.transpose(0, 1)\n",
        "\n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
        "\n",
        "    row = k_ix // k\n",
        "    col = k_ix % k\n",
        "\n",
        "    outputs[:, :i] = outputs[row, :i]\n",
        "    outputs[:, i] = ix[row, col]\n",
        "\n",
        "    log_scores = k_probs.unsqueeze(0)\n",
        "\n",
        "    return outputs, log_scores\n",
        "\n",
        "def beam_search(src, model, device, k, max_len):\n",
        "    \"\"\"\n",
        "    Beam search decoding (BPE version)\n",
        "    \"\"\"\n",
        "\n",
        "    outputs, e_outputs, log_scores = init_vars(\n",
        "        src, model, device, k, max_len\n",
        "    )\n",
        "\n",
        "    eos_tok = EOS_ID\n",
        "    src_mask = (src != PAD_ID).unsqueeze(-2)\n",
        "\n",
        "    ind = None\n",
        "\n",
        "    for i in range(2, max_len):\n",
        "\n",
        "        trg_mask = nopeak_mask(i, device)\n",
        "\n",
        "        out = model.out(\n",
        "            model.decoder(outputs[:, :i], e_outputs, src_mask, trg_mask)\n",
        "        )\n",
        "\n",
        "        out = F.softmax(out, dim=-1)\n",
        "\n",
        "        outputs, log_scores = k_best_outputs(\n",
        "            outputs, out, log_scores, i, k\n",
        "        )\n",
        "\n",
        "        # Check EOS positions\n",
        "        eos_positions = (outputs == eos_tok).nonzero(as_tuple=False)\n",
        "        sentence_lengths = torch.zeros(k, dtype=torch.long, device=device)\n",
        "\n",
        "        for row, col in eos_positions:\n",
        "            if sentence_lengths[row] == 0:\n",
        "                sentence_lengths[row] = col\n",
        "\n",
        "        num_finished = (sentence_lengths > 0).sum().item()\n",
        "\n",
        "        if num_finished == k:\n",
        "            alpha = 0.7\n",
        "            scores = log_scores / (sentence_lengths.float() ** alpha)\n",
        "            _, ind = scores.max(dim=1)\n",
        "            ind = ind.item()\n",
        "            break\n",
        "\n",
        "    # Decode best hypothesis\n",
        "    if ind is None:\n",
        "        best = outputs[0]\n",
        "    else:\n",
        "        best = outputs[ind]\n",
        "\n",
        "    # Cut at EOS if exists\n",
        "    eos_pos = (best == eos_tok).nonzero(as_tuple=False)\n",
        "    length = eos_pos[0].item() if len(eos_pos) > 0 else max_len\n",
        "\n",
        "    return bpe_tokenizer.decode(\n",
        "        best[1:length].tolist(),\n",
        "        skip_special_tokens=True\n",
        "    )"
      ],
      "metadata": {
        "id": "Vc0JX2zY7egp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, device, k, max_len):\n",
        "    \"\"\"\n",
        "    Translate one sentence using beam search (BPE version)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1. BPE tokenize → IDs\n",
        "    src_ids = bpe_tokenizer.encode(sentence).ids\n",
        "\n",
        "    # 2. Convert to tensor\n",
        "    src_tensor = torch.LongTensor([src_ids]).to(device)\n",
        "\n",
        "    # 3. Beam search\n",
        "    output = beam_search(\n",
        "        src_tensor,\n",
        "        model,\n",
        "        device=device,\n",
        "        k=k,\n",
        "        max_len=max_len\n",
        "    )\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "qZ1_6GJJ7gW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bpe_tokenize(text):\n",
        "    return bpe_tokenizer.encode(text).ids"
      ],
      "metadata": {
        "id": "ndC4DsvG7iw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext import data\n",
        "\n",
        "def create_fields():\n",
        "\n",
        "    SRC = data.Field(\n",
        "        tokenize=bpe_tokenize,\n",
        "        use_vocab=False,\n",
        "        pad_token=PAD_ID\n",
        "    )\n",
        "\n",
        "    TRG = data.Field(\n",
        "        tokenize=bpe_tokenize,\n",
        "        use_vocab=False,\n",
        "        pad_token=PAD_ID,\n",
        "        init_token=SOS_ID,\n",
        "        eos_token=EOS_ID\n",
        "    )\n",
        "\n",
        "    return SRC, TRG"
      ],
      "metadata": {
        "id": "r_R7q6xo7kHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = bpe_tokenizer.get_vocab_size()\n",
        "pad_idx = PAD_ID\n",
        "print(vocab_size)\n",
        "pad_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd2f3pgl7llq",
        "outputId": "29a51afd-7ee8-416e-b984-a54c9d9c4a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dill as pickle\n",
        "import pandas as pd\n",
        "\n",
        "def read_data(src_file, trg_file):\n",
        "    src_data = open(src_file).read().strip().split('\\n')\n",
        "\n",
        "    trg_data = open(trg_file).read().strip().split('\\n')\n",
        "\n",
        "    return src_data, trg_data\n",
        "\n",
        "def create_dataset(src_data, trg_data, max_len, batchsize, device, SRC, TRG, istrain=True):\n",
        "\n",
        "    raw_data = {\n",
        "        'src': src_data,\n",
        "        'trg': trg_data\n",
        "    }\n",
        "    df = pd.DataFrame(raw_data)\n",
        "\n",
        "    df = df[\n",
        "        df['src'].apply(lambda x: len(bpe_tokenize(x)) < max_len) &\n",
        "        df['trg'].apply(lambda x: len(bpe_tokenize(x)) < max_len)\n",
        "    ]\n",
        "\n",
        "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
        "\n",
        "    data_fields = [('src', SRC), ('trg', TRG)]\n",
        "    dataset = data.TabularDataset(\n",
        "        path=\"translate_transformer_temp.csv\",\n",
        "        format=\"csv\",\n",
        "        fields=data_fields\n",
        "    )\n",
        "\n",
        "    iterator = MyIterator(\n",
        "        dataset,\n",
        "        batch_size=batchsize,\n",
        "        device=device,\n",
        "        repeat=False,\n",
        "        sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "        batch_size_fn=batch_size_fn,\n",
        "        train=istrain,\n",
        "        shuffle=istrain\n",
        "    )\n",
        "\n",
        "    os.remove(\"translate_transformer_temp.csv\")\n",
        "    return iterator"
      ],
      "metadata": {
        "id": "FYpaAZ7Z7oWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = {\n",
        "    'train_src_data':'/content/VLSP_data/train_new.en',\n",
        "    'train_trg_data':'/content/VLSP_data/train_new.vi',\n",
        "    'valid_src_data':'/content/VLSP_data/valid.en',\n",
        "    'valid_trg_data':'/content/VLSP_data/valid.vi',\n",
        "    'src_lang':'en',\n",
        "    'trg_lang':'en',#'vi_spacy_model',\n",
        "    'max_strlen':160,\n",
        "    'batchsize':1500,\n",
        "    'device':'cuda',\n",
        "    'd_model': 512,\n",
        "    'n_layers': 6,\n",
        "    'heads': 8,\n",
        "    'dropout': 0.1,\n",
        "    'lr':0.0001,\n",
        "    'epochs':30,\n",
        "    'printevery': 200,\n",
        "    'k':5,\n",
        "    'd_ff': 2048\n",
        "}"
      ],
      "metadata": {
        "id": "eoDMcvkq9-A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_src_data, train_trg_data = read_data(opt['train_src_data'], opt['train_trg_data'])\n",
        "valid_src_data, valid_trg_data = read_data(opt['valid_src_data'], opt['valid_trg_data'])\n",
        "\n",
        "# SRC, TRG = create_fields(opt['src_lang'], opt['trg_lang'])\n",
        "SRC, TRG = create_fields()\n",
        "train_iter = create_dataset(train_src_data, train_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=True)\n",
        "valid_iter = create_dataset(valid_src_data, valid_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=False)"
      ],
      "metadata": {
        "id": "PDOVANwy-DaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_src_data, test_trg_data = read_data('/content/VLSP_data/public_test.en.txt', '/content/VLSP_data/public_test.vi.txt')"
      ],
      "metadata": {
        "id": "LCJL6Hu-srey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_pad = bpe_tokenizer.token_to_id(\"<pad>\")\n",
        "trg_pad = bpe_tokenizer.token_to_id(\"<pad>\")"
      ],
      "metadata": {
        "id": "xVmGpKXb-Mbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(model, optimizer, batch, criterion, step_num, device):\n",
        "    model.train()\n",
        "\n",
        "    # batch.src / batch.trg are already token IDs (BPE)\n",
        "    src = batch.src.transpose(0, 1).to(device)\n",
        "    trg = batch.trg.transpose(0, 1).to(device)\n",
        "\n",
        "    # Teacher forcing\n",
        "    trg_input = trg[:, :-1]\n",
        "    trg_gold = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "    # Masks (same PAD_ID for src & trg)\n",
        "    src_mask, trg_mask = create_masks(\n",
        "        src, trg_input, PAD_ID, PAD_ID, device\n",
        "    )\n",
        "\n",
        "    # Forward\n",
        "    preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "    # Loss\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(\n",
        "        preds.view(-1, preds.size(-1)),\n",
        "        trg_gold\n",
        "    )\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step_and_update_lr()\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss.item(),\n",
        "        \"step\": step_num + 1\n",
        "    }"
      ],
      "metadata": {
        "id": "hbwJhKn07tU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_iter, criterion, device):\n",
        "    \"\"\"Compute validation loss and perplexity.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_iter:\n",
        "            src = batch.src.transpose(0, 1).to(device)\n",
        "            trg = batch.trg.transpose(0, 1).to(device)\n",
        "\n",
        "            trg_input = trg[:, :-1]\n",
        "            trg_gold = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            src_mask, trg_mask = create_masks(\n",
        "                src, trg_input, PAD_ID, PAD_ID, device\n",
        "            )\n",
        "\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "            loss = criterion(\n",
        "                preds.view(-1, preds.size(-1)),\n",
        "                trg_gold\n",
        "            )\n",
        "\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "    avg_loss = float(np.mean(total_loss))\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return avg_loss, perplexity\n"
      ],
      "metadata": {
        "id": "Ge8TOAWJ7vx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.init_lr = init_lr\n",
        "        self.d_model = d_model\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_steps = 0\n",
        "\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients with the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        d_model = self.d_model\n",
        "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
        "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
        "\n",
        "    def state_dict(self):\n",
        "        optimizer_state_dict = {\n",
        "            'init_lr':self.init_lr,\n",
        "            'd_model':self.d_model,\n",
        "            'n_warmup_steps':self.n_warmup_steps,\n",
        "            'n_steps':self.n_steps,\n",
        "            '_optimizer':self._optimizer.state_dict(),\n",
        "        }\n",
        "\n",
        "        return optimizer_state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.init_lr = state_dict['init_lr']\n",
        "        self.d_model = state_dict['d_model']\n",
        "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
        "        self.n_steps = state_dict['n_steps']\n",
        "\n",
        "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "xdkftjOF_ZL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            # true_dist = pred.data.clone()\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 2))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "            true_dist[:, self.padding_idx] = 0\n",
        "            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "            if mask.dim() > 0:\n",
        "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ],
      "metadata": {
        "id": "2nZnBNrw_c8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(\n",
        "    vocab_size,          # src vocab size\n",
        "    vocab_size,          # trg vocab size (shared BPE)\n",
        "    opt['d_model'],\n",
        "    opt['n_layers'],\n",
        "    opt['heads'],\n",
        "    opt['d_ff'],\n",
        "    opt['dropout']\n",
        ")\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "model = model.to(opt['device'])"
      ],
      "metadata": {
        "id": "iporvlx2-O6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = ScheduledOptim(\n",
        "        torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "        1.0, opt['d_model'], 4000) #previously 0.2\n",
        "\n",
        "criterion = LabelSmoothingLoss(vocab_size, padding_idx=PAD_ID, smoothing=0.1)"
      ],
      "metadata": {
        "id": "WH252Jiq-RZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, step, best_bleu, path=\"checkpoint.pth\"):\n",
        "    checkpoint = {\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"epoch\": epoch,\n",
        "        \"step\": step,\n",
        "        \"best_bleu\": best_bleu,\n",
        "        \"rng_state\": torch.get_rng_state(),\n",
        "        \"cuda_rng_state\": torch.cuda.get_rng_state(),\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Checkpoint saved to {path}\")"
      ],
      "metadata": {
        "id": "xEgpnjsE-UEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def load_checkpoint(path, model, optimizer, device):\n",
        "    print(f\"Loading checkpoint from {path}...\")\n",
        "    try:\n",
        "        # Load the checkpoint onto the correct device\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Checkpoint file not found at {path}\")\n",
        "        return 0, 0, 0.0 # Return initial values if file doesn't exist\n",
        "\n",
        "    # ... (Restoring model, optimizer, epoch, step, best_bleu is omitted for brevity) ...\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "    step = checkpoint[\"step\"]\n",
        "    best_bleu = checkpoint[\"best_bleu\"]\n",
        "\n",
        "    # 5. Restore RNG states - CRITICAL FIX HERE\n",
        "    try:\n",
        "        # Convert the CPU RNG state to the expected torch.ByteTensor\n",
        "        cpu_rng_state = checkpoint[\"rng_state\"].type(torch.ByteTensor)\n",
        "        torch.set_rng_state(cpu_rng_state)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not restore CPU RNG state. Error: {e}\")\n",
        "\n",
        "    if 'cuda_rng_state' in checkpoint and torch.cuda.is_available():\n",
        "        try:\n",
        "            # Convert the CUDA RNG state to the expected torch.ByteTensor\n",
        "            cuda_rng_state = checkpoint[\"cuda_rng_state\"].type(torch.ByteTensor)\n",
        "            torch.cuda.set_rng_state(cuda_rng_state)\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: Could not restore CUDA RNG state. Error: {e}\")\n",
        "\n",
        "    print(f\"Checkpoint successfully loaded.\")\n",
        "    print(f\"  - Resuming from Epoch: {epoch}, Step: {step}\")\n",
        "    print(f\"  - Best BLEU Score recorded: {best_bleu:.4f}\")\n",
        "\n",
        "    return epoch, step, best_bleu"
      ],
      "metadata": {
        "id": "zY9jG_n2-WIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/VLSP_best_model.pth\""
      ],
      "metadata": {
        "id": "2w-u63qoAlIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch, start_step, best_bleu = load_checkpoint(\n",
        "    path=checkpoint_path,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    device=opt['device']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9VydYJoBQDk",
        "outputId": "7c210cc8-500c-4879-9af6-d47b5684089f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint from /content/drive/My Drive/VLSP_best_model.pth...\n",
            "Checkpoint successfully loaded.\n",
            "  - Resuming from Epoch: 2, Step: 37813\n",
            "  - Best BLEU Score recorded: 46.6980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 3          # stop after 5 epochs without BLEU improvement\n",
        "min_delta = 1e-4      # minimum BLEU improvement to count\n",
        "wait = 0              # how many epochs we have waited"
      ],
      "metadata": {
        "id": "IsSaYZCw-aZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Training steps and tools**"
      ],
      "metadata": {
        "id": "Ov7i45c48318"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Preparing for BLEU score calculation**"
      ],
      "metadata": {
        "id": "joJHO0Vc8NV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def detokenize(text: str) -> str:\n",
        "    # 1. Remove BPE markers\n",
        "    text = text.replace('@@ ', '').replace('@@', '')\n",
        "\n",
        "    # 2. Fix spacing before punctuation\n",
        "    text = re.sub(r'\\s+([?.!,;:])', r'\\1', text)\n",
        "\n",
        "    # 3. Fix quotes\n",
        "    text = re.sub(r\"\\s+'\", \"'\", text)\n",
        "    text = re.sub(r\"'\\s+\", \"'\", text)\n",
        "\n",
        "    # 4. Normalize spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "vR6b8qj_8Rsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "\n",
        "def bleu_sacre(valid_src_data, valid_trg_data, model, device, k=5, max_len=80):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sent in valid_src_data:\n",
        "            pred = translate_sentence(sent, model, device, k, max_len)\n",
        "            preds.append(detokenize(pred))\n",
        "\n",
        "    refs = [detokenize(ref) for ref in valid_trg_data]\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(preds, [refs])\n",
        "    return bleu.score"
      ],
      "metadata": {
        "id": "h7tjOaAZ75zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "step_num = 0\n",
        "best_bleu = 0\n",
        "\n",
        "# Training for RoPE + PreLayer Norm\n",
        "for epoch in range(start_epoch, opt['epochs']):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        s = time.time()\n",
        "\n",
        "        # step() now returns the loss only; step_num is tracked here\n",
        "        info = step(model, optimizer, batch, criterion, step_num, opt['device'])\n",
        "        loss = info['loss']\n",
        "        step_num = info['step']\n",
        "\n",
        "        total_loss += loss\n",
        "\n",
        "        if (i + 1) % opt['printevery'] == 0:\n",
        "            avg_loss = total_loss / opt['printevery']\n",
        "            print('epoch: {:03d} - iter: {:05d} - train loss: {:.4f} - time: {:.4f}'.format(\n",
        "                epoch, i, avg_loss, time.time() - s\n",
        "            ))\n",
        "            total_loss = 0\n",
        "\n",
        "    # Validation\n",
        "    s = time.time()\n",
        "    valid_loss, perplexity = validate(model, valid_iter, criterion, opt['device'])\n",
        "    bleuscore = bleu_sacre(\n",
        "        valid_src_data[:500], valid_trg_data[:500],\n",
        "        model, opt['device'], 1, opt['max_strlen']\n",
        "    )\n",
        "\n",
        "    print('epoch: {:03d} - iter: {:05d} - valid loss: {:.4f} - bleu score: {:.4f} - perplexity: {:.4f} - time: {:.4f}'.format(\n",
        "        epoch, i, valid_loss, bleuscore, perplexity, time.time() - s\n",
        "    ))\n",
        "\n",
        "    # ---- EARLY STOPPING LOGIC ----\n",
        "    if bleuscore > best_bleu + min_delta:\n",
        "        best_bleu = bleuscore\n",
        "        wait = 0\n",
        "\n",
        "        # save ONLY the best model\n",
        "        save_checkpoint(\n",
        "            path=\"VLSP_best_model.pth\",\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            epoch=epoch,\n",
        "            step=step_num,\n",
        "            best_bleu=best_bleu\n",
        "        )\n",
        "    else:\n",
        "        wait += 1\n",
        "        print(f\"No BLEU improvement for {wait}/{patience} epochs\")\n",
        "\n",
        "    if wait >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f-NCTK2K-6WP",
        "outputId": "fad51659-82bb-4938-88b5-9c67c6d2a0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 002 - iter: 00199 - train loss: 2.4238 - time: 0.2503\n",
            "epoch: 002 - iter: 00399 - train loss: 2.4355 - time: 0.2404\n",
            "epoch: 002 - iter: 00599 - train loss: 2.4247 - time: 0.2571\n",
            "epoch: 002 - iter: 00799 - train loss: 2.4316 - time: 0.2715\n",
            "epoch: 002 - iter: 00999 - train loss: 2.4491 - time: 0.2426\n",
            "epoch: 002 - iter: 01199 - train loss: 2.4343 - time: 0.2643\n",
            "epoch: 002 - iter: 01399 - train loss: 2.4289 - time: 0.2596\n",
            "epoch: 002 - iter: 01599 - train loss: 2.4162 - time: 0.2493\n",
            "epoch: 002 - iter: 01799 - train loss: 2.4582 - time: 0.2615\n",
            "epoch: 002 - iter: 01999 - train loss: 2.4274 - time: 0.2554\n",
            "epoch: 002 - iter: 02199 - train loss: 2.4320 - time: 0.2729\n",
            "epoch: 002 - iter: 02399 - train loss: 2.4588 - time: 0.2493\n",
            "epoch: 002 - iter: 02599 - train loss: 2.4565 - time: 0.2767\n",
            "epoch: 002 - iter: 02799 - train loss: 2.4622 - time: 0.2646\n",
            "epoch: 002 - iter: 02999 - train loss: 2.4649 - time: 0.2589\n",
            "epoch: 002 - iter: 03199 - train loss: 2.4523 - time: 0.2609\n",
            "epoch: 002 - iter: 03399 - train loss: 2.4521 - time: 0.2480\n",
            "epoch: 002 - iter: 03599 - train loss: 2.4446 - time: 0.2643\n",
            "epoch: 002 - iter: 03799 - train loss: 2.4656 - time: 0.2656\n",
            "epoch: 002 - iter: 03999 - train loss: 2.4448 - time: 0.2373\n",
            "epoch: 002 - iter: 04199 - train loss: 2.4383 - time: 0.2567\n",
            "epoch: 002 - iter: 04399 - train loss: 2.4590 - time: 0.2492\n",
            "epoch: 002 - iter: 04599 - train loss: 2.4636 - time: 0.2356\n",
            "epoch: 002 - iter: 04799 - train loss: 2.4350 - time: 0.2548\n",
            "epoch: 002 - iter: 04999 - train loss: 2.4350 - time: 0.2376\n",
            "epoch: 002 - iter: 05199 - train loss: 2.4637 - time: 0.2505\n",
            "epoch: 002 - iter: 05399 - train loss: 2.4464 - time: 0.2563\n",
            "epoch: 002 - iter: 05599 - train loss: 2.4500 - time: 0.2383\n",
            "epoch: 002 - iter: 05799 - train loss: 2.4459 - time: 0.2287\n",
            "epoch: 002 - iter: 05999 - train loss: 2.4260 - time: 0.2454\n",
            "epoch: 002 - iter: 06199 - train loss: 2.4475 - time: 0.2604\n",
            "epoch: 002 - iter: 06399 - train loss: 2.4645 - time: 0.2741\n",
            "epoch: 002 - iter: 06599 - train loss: 2.4546 - time: 0.2439\n",
            "epoch: 002 - iter: 06799 - train loss: 2.4543 - time: 0.2505\n",
            "epoch: 002 - iter: 06999 - train loss: 2.4478 - time: 0.2671\n",
            "epoch: 002 - iter: 07199 - train loss: 2.4383 - time: 0.2551\n",
            "epoch: 002 - iter: 07399 - train loss: 2.4507 - time: 0.2574\n",
            "epoch: 002 - iter: 07599 - train loss: 2.4111 - time: 0.2747\n",
            "epoch: 002 - iter: 07799 - train loss: 2.4410 - time: 0.2661\n",
            "epoch: 002 - iter: 07999 - train loss: 2.4247 - time: 0.2782\n",
            "epoch: 002 - iter: 08199 - train loss: 2.4271 - time: 0.2671\n",
            "epoch: 002 - iter: 08399 - train loss: 2.4563 - time: 0.2760\n",
            "epoch: 002 - iter: 08599 - train loss: 2.4476 - time: 0.2740\n",
            "epoch: 002 - iter: 08799 - train loss: 2.4492 - time: 0.2644\n",
            "epoch: 002 - iter: 08999 - train loss: 2.4377 - time: 0.2743\n",
            "epoch: 002 - iter: 09199 - train loss: 2.4469 - time: 0.2768\n",
            "epoch: 002 - iter: 09399 - train loss: 2.4303 - time: 0.2326\n",
            "epoch: 002 - iter: 09599 - train loss: 2.4490 - time: 0.2222\n",
            "epoch: 002 - iter: 09799 - train loss: 2.4317 - time: 0.2521\n",
            "epoch: 002 - iter: 09999 - train loss: 2.4387 - time: 0.2548\n",
            "epoch: 002 - iter: 10199 - train loss: 2.4564 - time: 0.2505\n",
            "epoch: 002 - iter: 10399 - train loss: 2.4221 - time: 0.2086\n",
            "epoch: 002 - iter: 10599 - train loss: 2.4348 - time: 0.2624\n",
            "epoch: 002 - iter: 10799 - train loss: 2.4266 - time: 0.2583\n",
            "epoch: 002 - iter: 10999 - train loss: 2.4505 - time: 0.2557\n",
            "epoch: 002 - iter: 11199 - train loss: 2.4643 - time: 0.2796\n",
            "epoch: 002 - iter: 11399 - train loss: 2.4258 - time: 0.2692\n",
            "epoch: 002 - iter: 11599 - train loss: 2.3900 - time: 0.2323\n",
            "epoch: 002 - iter: 11799 - train loss: 2.2987 - time: 0.2564\n",
            "epoch: 002 - iter: 11999 - train loss: 2.3120 - time: 0.2509\n",
            "epoch: 002 - iter: 12199 - train loss: 2.3391 - time: 0.2797\n",
            "epoch: 002 - iter: 12399 - train loss: 2.2952 - time: 0.2704\n",
            "epoch: 002 - iter: 12599 - train loss: 2.3606 - time: 0.2629\n",
            "epoch: 002 - iter: 12604 - valid loss: 2.0878 - bleu score: 47.6005 - perplexity: 8.0671 - time: 270.2212\n",
            "Checkpoint saved to VLSP_best_model.pth\n",
            "epoch: 003 - iter: 00199 - train loss: 2.3197 - time: 0.2577\n",
            "epoch: 003 - iter: 00399 - train loss: 2.3359 - time: 0.2603\n",
            "epoch: 003 - iter: 00599 - train loss: 2.3583 - time: 0.2704\n",
            "epoch: 003 - iter: 00799 - train loss: 2.3647 - time: 0.2562\n",
            "epoch: 003 - iter: 00999 - train loss: 2.3499 - time: 0.2741\n",
            "epoch: 003 - iter: 01199 - train loss: 2.3698 - time: 0.2831\n",
            "epoch: 003 - iter: 01399 - train loss: 2.3658 - time: 0.2700\n",
            "epoch: 003 - iter: 01599 - train loss: 2.3473 - time: 0.2671\n",
            "epoch: 003 - iter: 01799 - train loss: 2.3830 - time: 0.2770\n",
            "epoch: 003 - iter: 01999 - train loss: 2.3640 - time: 0.2710\n",
            "epoch: 003 - iter: 02199 - train loss: 2.3861 - time: 0.2575\n",
            "epoch: 003 - iter: 02399 - train loss: 2.3750 - time: 0.2463\n",
            "epoch: 003 - iter: 02599 - train loss: 2.3766 - time: 0.2593\n",
            "epoch: 003 - iter: 02799 - train loss: 2.3553 - time: 0.2759\n",
            "epoch: 003 - iter: 02999 - train loss: 2.3592 - time: 0.2536\n",
            "epoch: 003 - iter: 03199 - train loss: 2.3823 - time: 0.2441\n",
            "epoch: 003 - iter: 03399 - train loss: 2.4065 - time: 0.2446\n",
            "epoch: 003 - iter: 03599 - train loss: 2.3700 - time: 0.2586\n",
            "epoch: 003 - iter: 03799 - train loss: 2.3888 - time: 0.2469\n",
            "epoch: 003 - iter: 03999 - train loss: 2.3465 - time: 0.2454\n",
            "epoch: 003 - iter: 04199 - train loss: 2.3722 - time: 0.2687\n",
            "epoch: 003 - iter: 04399 - train loss: 2.3808 - time: 0.2753\n",
            "epoch: 003 - iter: 04599 - train loss: 2.3815 - time: 0.2524\n",
            "epoch: 003 - iter: 04799 - train loss: 2.3914 - time: 0.2588\n",
            "epoch: 003 - iter: 04999 - train loss: 2.3602 - time: 0.2565\n",
            "epoch: 003 - iter: 05199 - train loss: 2.3556 - time: 0.2539\n",
            "epoch: 003 - iter: 05399 - train loss: 2.3752 - time: 0.2479\n",
            "epoch: 003 - iter: 05599 - train loss: 2.3689 - time: 0.2566\n",
            "epoch: 003 - iter: 05799 - train loss: 2.3880 - time: 0.2621\n",
            "epoch: 003 - iter: 05999 - train loss: 2.3687 - time: 0.2559\n",
            "epoch: 003 - iter: 06199 - train loss: 2.3753 - time: 0.2428\n",
            "epoch: 003 - iter: 06399 - train loss: 2.3663 - time: 0.2516\n",
            "epoch: 003 - iter: 06599 - train loss: 2.3672 - time: 0.2715\n",
            "epoch: 003 - iter: 06799 - train loss: 2.3877 - time: 0.2555\n",
            "epoch: 003 - iter: 06999 - train loss: 2.3886 - time: 0.2558\n",
            "epoch: 003 - iter: 07199 - train loss: 2.3853 - time: 0.2597\n",
            "epoch: 003 - iter: 07399 - train loss: 2.3770 - time: 0.2429\n",
            "epoch: 003 - iter: 07599 - train loss: 2.3715 - time: 0.2657\n",
            "epoch: 003 - iter: 07799 - train loss: 2.3794 - time: 0.2743\n",
            "epoch: 003 - iter: 07999 - train loss: 2.3628 - time: 0.2632\n",
            "epoch: 003 - iter: 08199 - train loss: 2.3420 - time: 0.2430\n",
            "epoch: 003 - iter: 08399 - train loss: 2.3672 - time: 0.2511\n",
            "epoch: 003 - iter: 08599 - train loss: 2.3863 - time: 0.2509\n",
            "epoch: 003 - iter: 08799 - train loss: 2.3902 - time: 0.2531\n",
            "epoch: 003 - iter: 08999 - train loss: 2.3959 - time: 0.2806\n",
            "epoch: 003 - iter: 09199 - train loss: 2.3640 - time: 0.2600\n",
            "epoch: 003 - iter: 09399 - train loss: 2.3901 - time: 0.2579\n",
            "epoch: 003 - iter: 09599 - train loss: 2.3786 - time: 0.2769\n",
            "epoch: 003 - iter: 09799 - train loss: 2.3555 - time: 0.2445\n",
            "epoch: 003 - iter: 09999 - train loss: 2.3525 - time: 0.2423\n",
            "epoch: 003 - iter: 10199 - train loss: 2.3656 - time: 0.2648\n",
            "epoch: 003 - iter: 10399 - train loss: 2.3993 - time: 0.2507\n",
            "epoch: 003 - iter: 10599 - train loss: 2.3772 - time: 0.2439\n",
            "epoch: 003 - iter: 10799 - train loss: 2.3748 - time: 0.2347\n",
            "epoch: 003 - iter: 10999 - train loss: 2.3700 - time: 0.2782\n",
            "epoch: 003 - iter: 11199 - train loss: 2.3640 - time: 0.2552\n",
            "epoch: 003 - iter: 11399 - train loss: 2.4061 - time: 0.2550\n",
            "epoch: 003 - iter: 11599 - train loss: 2.3770 - time: 0.2517\n",
            "epoch: 003 - iter: 11799 - train loss: 2.2533 - time: 0.2554\n",
            "epoch: 003 - iter: 11999 - train loss: 2.2602 - time: 0.2281\n",
            "epoch: 003 - iter: 12199 - train loss: 2.2893 - time: 0.2578\n",
            "epoch: 003 - iter: 12399 - train loss: 2.2363 - time: 0.2060\n",
            "epoch: 003 - iter: 12599 - train loss: 2.2460 - time: 0.2632\n",
            "epoch: 003 - iter: 12612 - valid loss: 2.0614 - bleu score: 46.7691 - perplexity: 7.8571 - time: 264.3173\n",
            "No BLEU improvement for 1/3 epochs\n",
            "epoch: 004 - iter: 00199 - train loss: 2.3027 - time: 0.2724\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-339418786.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# step() now returns the loss only; step_num is tracked here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstep_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1205307522.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(model, optimizer, batch, criterion, step_num, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_and_update_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     return {\n",
            "\u001b[0;32m/tmp/ipython-input-353564101.py\u001b[0m in \u001b[0;36mstep_and_update_lr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m\"Step with the inner optimizer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    954\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             bias_correction1 = [\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             ]\n\u001b[1;32m    761\u001b[0m             bias_correction2 = [\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwOnzO-Sqp6o",
        "outputId": "80e3be02-fb52-4c14-e9c8-cc71f70e238f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjYVaCIX-RTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVvtk4Pd-RVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/VLSP_best_model_final.pth /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "d4VMboSQqx2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(\n",
        "    model.state_dict(),\n",
        "    \"/content/drive/MyDrive/VLSP_model_dict.pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "kTkDQ_M4rQw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence='The determinants of knowledge and practices in public health service utilization among health insurance card''s holders were distance and time taken to health services, time of health insurance and health insurance information provided.'\n",
        "trans_sent = translate_sentence(sentence, model, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "EHwas2GTranR",
        "outputId": "e42bca07-e29b-41f7-b428-598ebcb07a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Các yếu tố liên quan đến kiến thức, thực hành sử dụng dịch vụ y tế công cộng của người dân bảo hiểm y tế là khoảng cách và thời gian tham gia dịch vụ y tế, thời gian bảo hiểm y tế và thông tin bảo hiểm y tế được cung cấp.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence='The purpose of this study was to evaluate the effects of a mixture extract of C chrysantha and G pentaphyllum on weight loss and lowering lipid blood levels in obese Swiss mice.'\n",
        "trans_sent = translate_sentence(sentence, model, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "z3KaJ1Vur5dI",
        "outputId": "78145964-67f3-4e95-d9f9-70460986aabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mục đích của nghiên cứu này là đánh giá tác động của hỗn hợp cao phối hợp C. chrysantha và G. tuần hoàn trong việc giảm cân và hạ lipid máu trên chuột nhắt trắng bị béo phì.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence='Mice in each group was assessed for weight weekly and the levels of Total Cholesterol (CT), HDLCholesterol (HDL-C), LDL-Cholesterol (LDL-C) and Triglyceride (TC) was recorded at initial time (after obesity was induced for 8 weeks) and 1 hour after taking the extracted mixtures on the last day.'\n",
        "trans_sent = translate_sentence(sentence, model, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "bSxmPUx1sH7k",
        "outputId": "008f74a3-411e-471d-f72f-c1ac007b302f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mỗi nhóm được đánh giá cân nặng hàng tuần và nồng độ Cholesterol toàn phần (CT), HDL cholesterol (HDL-C), LDL-C (LDL-C) và triglyceride (TC) được ghi nhận vào thời điểm ban đầu (sau khi gây béo phì trong 8 tuần) và 1 giờ sau khi uống hỗn hợp dịch chiết vào ngày cuối.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence='Conclusion: The proportion of proton pump inhibitors was not safe and reasonable and the proportion of prescription drugs with no instructions on how long to use proton pump inhibitors were low. The proportion of prescription interacting drugs accounted for a high proportion, clopidogrel was the most interactive drug commonly used with PPIs.'\n",
        "trans_sent = translate_sentence(sentence, model, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "qHQ6LvyxsV6S",
        "outputId": "ada0c0ec-94d4-40d9-9fe9-d370bacbbe1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Kết luận: Tỷ lệ sử dụng thuốc ức chế bơm proton không an toàn và hợp lý và tỷ lệ thuốc theo toa không có hướng dẫn sử dụng thuốc ức chế bơm proton còn thấp, tỷ lệ các thuốc có tác dụng tương tác thuốc chiếm tỷ lệ cao, clopidogrel là thuốc tương tác được sử dụng phổ biến nhất với PPI.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_src_data, test_trg_data = read_data('', '/content/KC4.0_MultilingualNMT/data/iwslt_en_vi/tst2013.vi')"
      ],
      "metadata": {
        "id": "kAqnewOvsC_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = bleu_sacre(test_src_data[:10000], test_trg_data[:10000], model, opt['device'], opt['k'], opt['max_strlen'])\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33fpMskp8XTR",
        "outputId": "761a139c-c9dc-4e03-f4e6-e8e8cf37d02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45.276843593577155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "import torch\n",
        "\n",
        "def ter_sacre(valid_src_data, valid_trg_data, model, device, k=5, max_len=80):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sent in valid_src_data:\n",
        "            pred = translate_sentence(sent, model, device, k, max_len)\n",
        "            preds.append(detokenize(pred))\n",
        "\n",
        "    refs = [detokenize(ref) for ref in valid_trg_data]\n",
        "\n",
        "    ter = sacrebleu.metrics.TER()\n",
        "    score = ter.corpus_score(preds, [refs])\n",
        "\n",
        "    return score.score  # lower is better\n"
      ],
      "metadata": {
        "id": "oMyPAswas6tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ter_score = ter_sacre(test_src_data[:1000], test_trg_data[:1000], model, opt['device'])\n",
        "print(\"TER:\", ter_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCkoV8tjvgTj",
        "outputId": "c283da32-2f8f-469a-dc8a-9eaf8167f1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TER: 46.93931837073982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McVp4XP6vIah",
        "outputId": "06aa2069-2358-4592-cc48-d8f07dd5ae13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKyI8C4rvKG8",
        "outputId": "2621873e-2e56-4e42-953a-b34ca3825de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_for_meteor(text):\n",
        "    return text.split()"
      ],
      "metadata": {
        "id": "pxPmdz722779"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detokenize_pred(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Minimal detokenization for model predictions:\n",
        "    - Remove BPE continuation markers\n",
        "    - Do NOT normalize punctuation or spacing\n",
        "    \"\"\"\n",
        "    return text.replace('@@ ', '').replace('@@', '').strip()\n"
      ],
      "metadata": {
        "id": "dN-tEnzU3Jci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import meteor_score\n",
        "import torch\n",
        "\n",
        "def meteor_corpus(src_data, trg_data, model, device, k=5, max_len=80):\n",
        "    model.eval()\n",
        "    scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, ref in zip(src_data, trg_data):\n",
        "            pred = translate_sentence(src, model, device, k, max_len)\n",
        "\n",
        "            pred_text = detokenize_pred(pred)\n",
        "            ref_text = ref  # original reference text\n",
        "\n",
        "            pred_tokens = pred_text.split()\n",
        "            ref_tokens = ref_text.split()\n",
        "\n",
        "            score = meteor_score([ref_tokens], pred_tokens)\n",
        "            scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "DilmOyG5vNLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteor = meteor_corpus(test_src_data[:1000], test_trg_data[:1000], model, opt['device'])\n",
        "print(\"METEOR:\", meteor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGH0ZhISvNNX",
        "outputId": "21dbc07a-9da2-4cf7-bd2b-d25b798e5823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR: 0.6612023580010491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vl6H4-ie2u2g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}