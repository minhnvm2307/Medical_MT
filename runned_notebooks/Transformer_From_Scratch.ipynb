{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c74ac0a3428844d38feecd4fc2cc6de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5eedcd43e2f4657bb4e5ca39bd9ccc1"
            ],
            "layout": "IPY_MODEL_ca37b8149f7147628a6f6412669919e2"
          }
        },
        "9a12ed9cf8bb48ef8ed05392988577c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_365a2b2beb5045c2842ed8b8423b9356",
            "placeholder": "​",
            "style": "IPY_MODEL_629407ccfcbf4de28af871fb5a75f21e",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "601fcf1a903f4f52a8df4c4d0fae5dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b16abc72f2c6428997997f83a37b8e72",
            "placeholder": "​",
            "style": "IPY_MODEL_abcaf1f89c2e4ef48dbeb5a729eb373e",
            "value": "kwahnguyen"
          }
        },
        "ffb06fbcaeb7439d9086d53ae88a2a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7b54d524ff92466eb0e937b970300fd7",
            "placeholder": "​",
            "style": "IPY_MODEL_17774d1e24ff409293f6b65513737cf5",
            "value": ""
          }
        },
        "75eb6e4bd7c14066a42b938f53cd8d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ab2699c596e84f2fbaabaa3d0b27a17a",
            "style": "IPY_MODEL_e46012acabb74960825a1eca14ec6b7f",
            "tooltip": ""
          }
        },
        "1d74874a06dc4bcb863771b6ec44d225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e89c1774dbf4d9b8c159b4245b871f7",
            "placeholder": "​",
            "style": "IPY_MODEL_06b2af29d66946009d39bb5c6898578b",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "ca37b8149f7147628a6f6412669919e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "365a2b2beb5045c2842ed8b8423b9356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "629407ccfcbf4de28af871fb5a75f21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b16abc72f2c6428997997f83a37b8e72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abcaf1f89c2e4ef48dbeb5a729eb373e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b54d524ff92466eb0e937b970300fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17774d1e24ff409293f6b65513737cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab2699c596e84f2fbaabaa3d0b27a17a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e46012acabb74960825a1eca14ec6b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7e89c1774dbf4d9b8c159b4245b871f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06b2af29d66946009d39bb5c6898578b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "508e87933d134dd49c76e1ae106e5baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ab0338241b477bab1d31db7f6ddeae",
            "placeholder": "​",
            "style": "IPY_MODEL_b187b1bea37342a8b0bc797855e025e5",
            "value": "Connecting..."
          }
        },
        "33ab0338241b477bab1d31db7f6ddeae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b187b1bea37342a8b0bc797855e025e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5eedcd43e2f4657bb4e5ca39bd9ccc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ced265979c394f898e45314f23d8d915",
            "placeholder": "​",
            "style": "IPY_MODEL_d318c620a7b14207b92b9950474be782",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "ced265979c394f898e45314f23d8d915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d318c620a7b14207b92b9950474be782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **Requirements**"
      ],
      "metadata": {
        "id": "cO90WWWMSVMu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hNEh7Yh8ReQ5",
        "outputId": "b7e5ad61-0682-4434-a9bd-8ad6e2d116cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m149.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        }
      ],
      "source": [
        "! pip -q install torchtext==0.6.0\n",
        "! pip -q install pyvi\n",
        "! python -m spacy download en_core_web_sm\n",
        "! pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjOi7YLrYJLw",
        "outputId": "94533f4c-4585-4961-9bd3-f960240880ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZwGiSeWSnzM",
        "outputId": "53212db4-c139-43ef-cc92-3aa98416ea6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sublayer's Components**\n",
        "- Embedding\n",
        "- LayerNorm\n",
        "- Feed Forward Network\n"
      ],
      "metadata": {
        "id": "j25zaYiRpnOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output: Shape ()\n",
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "metadata": {
        "id": "NeA3vJrSSsrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.size = d_model\n",
        "\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "metadata": {
        "id": "uI18ExGeqfVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "q51h8ajoqmFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=300, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        for pos in range(max_seq_length):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos/(10000**(2*i/d_model)))\n",
        "                pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x*math.sqrt(self.d_model)\n",
        "        seq_length = x.size(1)\n",
        "\n",
        "        # Old code\n",
        "        # pe = Variable(self.pe[:, :seq_length], requires_grad=False)\n",
        "\n",
        "        pe = self.pe[:, :seq_length].to(x.device)\n",
        "\n",
        "        if x.is_cuda:\n",
        "            pe.cuda()\n",
        "        # cộng embedding vector với pe\n",
        "        x = x + pe\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# PositionalEncoder(512)(torch.rand(5, 30, 512)).shape"
      ],
      "metadata": {
        "id": "O_YLDddu0PHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "metadata": {
        "id": "185VIMAYqzgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def nopeak_mask(size, device):\n",
        "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
        "     mô hình không nhìn thấy được các từ ở tương lai\n",
        "    \"\"\"\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\n",
        "    k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    np_mask = np_mask.to(device)\n",
        "\n",
        "    return np_mask\n",
        "\n",
        "def create_masks(src, trg, src_pad, trg_pad, device):\n",
        "    \"\"\" Tạo mask cho encoder,\n",
        "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào\n",
        "    \"\"\"\n",
        "    src_mask = (src != src_pad).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size, device)\n",
        "        if trg.is_cuda:\n",
        "            np_mask.cuda()\n",
        "        trg_mask = trg_mask & np_mask\n",
        "\n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ],
      "metadata": {
        "id": "2QPrQsGfwb9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Attention mechanism**"
      ],
      "metadata": {
        "id": "uLblfND6rFs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(q, k, v, mask=None, dropout=None):\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "\n",
        "    output = torch.matmul(scores, v)\n",
        "    return output, scores\n",
        "\n",
        "# attention(torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512)).shape"
      ],
      "metadata": {
        "id": "EjlnRk2qTWcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model//heads\n",
        "        self.h = heads\n",
        "        self.attn = None\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        bs = q.size(0)\n",
        "\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        scores, self.attn = attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
        "\n",
        "        output = self.out(concat)\n",
        "        return output"
      ],
      "metadata": {
        "id": "tttWfkbWpYvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Model architecture**"
      ],
      "metadata": {
        "id": "J1Nimnufwjzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# EncoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32 , 1, 30)).shape"
      ],
      "metadata": {
        "id": "B4ZCpLudpjZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "SZGXx1sKr3Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# DecoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
      ],
      "metadata": {
        "id": "AZX2JdpnvbcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Decoder(232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
      ],
      "metadata": {
        "id": "_nBy6d2SvkVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "\n",
        "# Transformer(232, 232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.LongTensor(32, 30).random_(0, 10),torch.rand(32, 1, 30),torch.rand(32, 1, 30)).shape"
      ],
      "metadata": {
        "id": "L4IlEOEQvu6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Training helpers**"
      ],
      "metadata": {
        "id": "sRRA86wIwMvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext import data\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "\n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)\n"
      ],
      "metadata": {
        "id": "b-q5-naZwMNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "\n",
        "def get_synonym(word, SRC):\n",
        "    syns = wordnet.synsets(word)\n",
        "    for s in syns:\n",
        "        for l in s.lemmas():\n",
        "            if SRC.vocab.stoi[l.name()] != 0:\n",
        "                return SRC.vocab.stoi[l.name()]\n",
        "\n",
        "    return 0\n",
        "\n",
        "def multiple_replace(dict, text):\n",
        "  # Create a regular expression  from the dictionary keys\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
        "\n",
        "  # For each match, look-up corresponding value in dictionary\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
      ],
      "metadata": {
        "id": "G-cyE5Rpwnea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_vars(src, model, SRC, TRG, device, k, max_len):\n",
        "    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n",
        "    \"\"\"\n",
        "    init_tok = TRG.vocab.stoi['<sos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "\n",
        "    # tính sẵn output của encoder\n",
        "    e_output = model.encoder(src, src_mask)\n",
        "\n",
        "    outputs = torch.LongTensor([[init_tok]])\n",
        "\n",
        "    outputs = outputs.to(device)\n",
        "\n",
        "    trg_mask = nopeak_mask(1, device)\n",
        "    # dự đoán kí tự đầu tiên\n",
        "    out = model.out(model.decoder(outputs,\n",
        "    e_output, src_mask, trg_mask))\n",
        "    out = F.softmax(out, dim=-1)\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
        "\n",
        "    outputs = torch.zeros(k, max_len).long()\n",
        "    outputs = outputs.to(device)\n",
        "    outputs[:, 0] = init_tok\n",
        "    outputs[:, 1] = ix[0]\n",
        "\n",
        "    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n",
        "\n",
        "    e_outputs = e_outputs.to(device)\n",
        "    e_outputs[:, :] = e_output[0]\n",
        "\n",
        "    return outputs, e_outputs, log_scores\n",
        "\n",
        "def k_best_outputs(outputs, out, log_scores, i, k):\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
        "\n",
        "    row = k_ix // k\n",
        "    col = k_ix % k\n",
        "\n",
        "    outputs[:, :i] = outputs[row, :i]\n",
        "    outputs[:, i] = ix[row, col]\n",
        "\n",
        "    log_scores = k_probs.unsqueeze(0)\n",
        "\n",
        "    return outputs, log_scores"
      ],
      "metadata": {
        "id": "Wym8adWRwvln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(src, model, SRC, TRG, device, k, max_len):\n",
        "\n",
        "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n",
        "    eos_tok = TRG.vocab.stoi['<eos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "    ind = None\n",
        "    for i in range(2, max_len):\n",
        "\n",
        "        trg_mask = nopeak_mask(i, device)\n",
        "\n",
        "        out = model.out(model.decoder(outputs[:,:i],\n",
        "        e_outputs, src_mask, trg_mask))\n",
        "\n",
        "        out = F.softmax(out, dim=-1)\n",
        "\n",
        "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n",
        "\n",
        "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
        "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
        "        for vec in ones:\n",
        "            i = vec[0]\n",
        "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
        "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
        "\n",
        "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
        "\n",
        "        if num_finished_sentences == k:\n",
        "            alpha = 0.7\n",
        "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
        "            _, ind = torch.max(log_scores * div, 1)\n",
        "            ind = ind.data[0]\n",
        "            break\n",
        "\n",
        "    if ind is None:\n",
        "\n",
        "        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
        "\n",
        "    else:\n",
        "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
      ],
      "metadata": {
        "id": "n23dE9yGw0TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, SRC, TRG, device, k, max_len):\n",
        "    \"\"\"Dịch một câu sử dụng beamsearch\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    indexed = []\n",
        "    sentence = SRC.preprocess(sentence)\n",
        "\n",
        "    for tok in sentence:\n",
        "        if SRC.vocab.stoi[tok] != SRC.vocab.stoi['<eos>']:\n",
        "            indexed.append(SRC.vocab.stoi[tok])\n",
        "        else:\n",
        "            indexed.append(get_synonym(tok, SRC))\n",
        "\n",
        "    sentence = Variable(torch.LongTensor([indexed]))\n",
        "\n",
        "    sentence = sentence.to(device)\n",
        "\n",
        "    sentence = beam_search(sentence, model, SRC, TRG, device, k, max_len)\n",
        "\n",
        "    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
      ],
      "metadata": {
        "id": "Vjz9-ZhHw39R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "class tokenize(object):\n",
        "\n",
        "    def __init__(self, lang):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def tokenizer(self, sentence):\n",
        "        sentence = re.sub(\n",
        "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "        sentence = sentence.lower()\n",
        "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
      ],
      "metadata": {
        "id": "ylKf81oZw522"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dill as pickle\n",
        "import pandas as pd\n",
        "\n",
        "def read_data(src_file, trg_file):\n",
        "    src_data = open(src_file).read().strip().split('\\n')\n",
        "\n",
        "    trg_data = open(trg_file).read().strip().split('\\n')\n",
        "\n",
        "    return src_data, trg_data\n",
        "\n",
        "def create_fields(src_lang, trg_lang):\n",
        "\n",
        "    print(\"loading spacy tokenizers...\")\n",
        "\n",
        "    t_src = tokenize(src_lang)\n",
        "    t_trg = tokenize(trg_lang)\n",
        "\n",
        "    TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n",
        "    SRC = data.Field(lower=True, tokenize=t_src.tokenizer)\n",
        "\n",
        "    return SRC, TRG\n",
        "\n",
        "def create_dataset(src_data, trg_data, max_strlen, batchsize, device, SRC, TRG, istrain=True):\n",
        "\n",
        "    print(\"creating dataset and iterator... \")\n",
        "\n",
        "    raw_data = {'src' : [line for line in src_data], 'trg': [line for line in trg_data]}\n",
        "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
        "\n",
        "    mask = (df['src'].str.count(' ') < max_strlen) & (df['trg'].str.count(' ') < max_strlen)\n",
        "    df = df.loc[mask]\n",
        "\n",
        "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
        "\n",
        "    data_fields = [('src', SRC), ('trg', TRG)]\n",
        "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
        "\n",
        "    train_iter = MyIterator(train, batch_size=batchsize, device=device,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=istrain, shuffle=True)\n",
        "\n",
        "    os.remove('translate_transformer_temp.csv')\n",
        "\n",
        "    if istrain:\n",
        "        SRC.build_vocab(train)\n",
        "        TRG.build_vocab(train)\n",
        "\n",
        "    return train_iter"
      ],
      "metadata": {
        "id": "b3zIHcM3w9V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(model, optimizer, batch, criterion, step_num):\n",
        "    model.train()\n",
        "\n",
        "    src = batch.src.transpose(0,1).cuda()\n",
        "    trg = batch.trg.transpose(0,1).cuda()\n",
        "\n",
        "    trg_input = trg[:, :-1]\n",
        "    src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
        "\n",
        "    preds = model(src, trg_input, src_mask, trg_mask)\n",
        "    ys = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "    loss.backward()\n",
        "    optimizer.step_and_update_lr()\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss.item(),\n",
        "        \"step\": step_num + 1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "YhJW2kN-w_EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, step, best_bleu, path=\"checkpoint.pth\"):\n",
        "    checkpoint = {\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"epoch\": epoch,\n",
        "        \"step\": step,\n",
        "        \"best_bleu\": best_bleu,\n",
        "        \"rng_state\": torch.get_rng_state(),\n",
        "        \"cuda_rng_state\": torch.cuda.get_rng_state(),\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Checkpoint saved to {path}\")\n"
      ],
      "metadata": {
        "id": "jENoIzWYyomd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(path, model, optimizer):\n",
        "    checkpoint = torch.load(path)\n",
        "\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "    step = checkpoint[\"step\"]\n",
        "    best_bleu = checkpoint[\"best_bleu\"]\n",
        "\n",
        "    torch.set_rng_state(checkpoint[\"rng_state\"])\n",
        "    torch.cuda.set_rng_state(checkpoint[\"cuda_rng_state\"])\n",
        "\n",
        "    print(f\"Checkpoint loaded from {path}\")\n",
        "    return epoch, step, best_bleu\n"
      ],
      "metadata": {
        "id": "SRshocVUypgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_iter, criterion):\n",
        "    \"\"\"Compute validation loss and perplexity.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_iter:\n",
        "            src = batch.src.transpose(0, 1).cuda()\n",
        "            trg = batch.trg.transpose(0, 1).cuda()\n",
        "\n",
        "            trg_input = trg[:, :-1]\n",
        "\n",
        "            src_mask, trg_mask = create_masks(\n",
        "                src, trg_input, src_pad, trg_pad, opt['device']\n",
        "            )\n",
        "\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "            ys = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "    avg_loss = np.mean(total_loss)\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return avg_loss, perplexity"
      ],
      "metadata": {
        "id": "Xv63JtBwxAhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Building Optimizer**"
      ],
      "metadata": {
        "id": "KsYunHxpxCwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.init_lr = init_lr\n",
        "        self.d_model = d_model\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_steps = 0\n",
        "\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients with the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        d_model = self.d_model\n",
        "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
        "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
        "\n",
        "    def state_dict(self):\n",
        "        optimizer_state_dict = {\n",
        "            'init_lr':self.init_lr,\n",
        "            'd_model':self.d_model,\n",
        "            'n_warmup_steps':self.n_warmup_steps,\n",
        "            'n_steps':self.n_steps,\n",
        "            '_optimizer':self._optimizer.state_dict(),\n",
        "        }\n",
        "\n",
        "        return optimizer_state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.init_lr = state_dict['init_lr']\n",
        "        self.d_model = state_dict['d_model']\n",
        "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
        "        self.n_steps = state_dict['n_steps']\n",
        "\n",
        "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "ly0zv2_oxCXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Label Smoothing**"
      ],
      "metadata": {
        "id": "7lvofp_FxJIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            # true_dist = pred.data.clone()\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 2))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "            true_dist[:, self.padding_idx] = 0\n",
        "            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "            if mask.dim() > 0:\n",
        "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ],
      "metadata": {
        "id": "UTwkzpn5xIjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "\n",
        "# Computing BLEU score\n",
        "def bleu_sacre(valid_src_data, valid_trg_data, model, SRC, TRG, device, k, max_strlen):\n",
        "    pred_sents = []\n",
        "\n",
        "    # 1. Generate predictions\n",
        "    for sentence in valid_src_data:\n",
        "        pred_trg = translate_sentence(sentence, model, SRC, TRG, device, k, max_strlen)\n",
        "        pred_sents.append(pred_trg)\n",
        "\n",
        "    # 2. Detokenize predictions (depends on your translate_sentence output)\n",
        "    # If pred_trg is already a string, skip this.\n",
        "    # pred_sents = [\" \".join(TRG.preprocess(sent)) if isinstance(sent, str) else \" \".join(sent)\n",
        "    #               for sent in pred_sents]\n",
        "\n",
        "    # 3. Prepare reference sentences\n",
        "    references = [\" \".join([tok for tok in TRG.preprocess(ref)]) for ref in valid_trg_data]\n",
        "\n",
        "    # 4. Compute sacreBLEU\n",
        "    bleu = sacrebleu.corpus_bleu(pred_sents, [references])\n",
        "\n",
        "    return bleu.score\n"
      ],
      "metadata": {
        "id": "NBuSA4eNxYzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Preparing training data**"
      ],
      "metadata": {
        "id": "knUkdVKexfDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "c74ac0a3428844d38feecd4fc2cc6de9",
            "9a12ed9cf8bb48ef8ed05392988577c2",
            "601fcf1a903f4f52a8df4c4d0fae5dd3",
            "ffb06fbcaeb7439d9086d53ae88a2a1c",
            "75eb6e4bd7c14066a42b938f53cd8d73",
            "1d74874a06dc4bcb863771b6ec44d225",
            "ca37b8149f7147628a6f6412669919e2",
            "365a2b2beb5045c2842ed8b8423b9356",
            "629407ccfcbf4de28af871fb5a75f21e",
            "b16abc72f2c6428997997f83a37b8e72",
            "abcaf1f89c2e4ef48dbeb5a729eb373e",
            "7b54d524ff92466eb0e937b970300fd7",
            "17774d1e24ff409293f6b65513737cf5",
            "ab2699c596e84f2fbaabaa3d0b27a17a",
            "e46012acabb74960825a1eca14ec6b7f",
            "7e89c1774dbf4d9b8c159b4245b871f7",
            "06b2af29d66946009d39bb5c6898578b",
            "508e87933d134dd49c76e1ae106e5baf",
            "33ab0338241b477bab1d31db7f6ddeae",
            "b187b1bea37342a8b0bc797855e025e5",
            "e5eedcd43e2f4657bb4e5ca39bd9ccc1",
            "ced265979c394f898e45314f23d8d915",
            "d318c620a7b14207b92b9950474be782"
          ]
        },
        "id": "am93JrA2xg7v",
        "outputId": "2996db00-80f6-4af5-cc9b-25576ae423e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c74ac0a3428844d38feecd4fc2cc6de9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run to use VLSP dataset\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nguynvitcng21020173/vlsp-2025-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V126c04Fxqim",
        "outputId": "0ebeec90-dd3c-416f-c9f7-8b538bc48d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/nguynvitcng21020173/vlsp-2025-data?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.1M/49.1M [00:00<00:00, 216MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/nguynvitcng21020173/vlsp-2025-data/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /root/.cache/kagglehub/datasets/nguynvitcng21020173/vlsp-2025-data/versions/1 /content/VLSP_data"
      ],
      "metadata": {
        "id": "rcfU4FRsxsDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 20000 '/content/VLSP_data/train.en.txt' > '/content/VLSP_data/valid.en'\n",
        "!head -n 20000 '/content/VLSP_data/train.vi.txt' > '/content/VLSP_data/valid.vi'\n",
        "!tail -n +20001 '/content/VLSP_data/train.en.txt' > '/content/VLSP_data/train_new.en'\n",
        "!tail -n +20001 '/content/VLSP_data/train.vi.txt' > '/content/VLSP_data/train_new.vi'"
      ],
      "metadata": {
        "id": "FmIvxME-xvES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = {\n",
        "    'train_src_data':'/content/VLSP_data/train_new.en',\n",
        "    'train_trg_data':'/content/VLSP_data/train_new.vi',\n",
        "    'valid_src_data':'/content/VLSP_data/valid.en',\n",
        "    'valid_trg_data':'/content/VLSP_data/valid.vi',\n",
        "    'src_lang':'en',\n",
        "    'trg_lang':'en',\n",
        "    'max_strlen':160,\n",
        "    'batchsize':1500,\n",
        "    'device':'cuda',\n",
        "    'd_model': 512,\n",
        "    'n_layers': 6,\n",
        "    'heads': 8,\n",
        "    'dropout': 0.1,\n",
        "    'lr':0.0001,\n",
        "    'epochs':30,\n",
        "    'printevery': 200,\n",
        "    'k':5,\n",
        "}"
      ],
      "metadata": {
        "id": "tNqMjEqCxzNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_src_data, train_trg_data = read_data(opt['train_src_data'], opt['train_trg_data'])\n",
        "valid_src_data, valid_trg_data = read_data(opt['valid_src_data'], opt['valid_trg_data'])\n",
        "\n",
        "SRC, TRG = create_fields(opt['src_lang'], opt['trg_lang'])\n",
        "train_iter = create_dataset(train_src_data, train_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=True)\n",
        "valid_iter = create_dataset(valid_src_data, valid_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6hAXqKix8cg",
        "outputId": "933b1f5e-7134-4d3d-a769-4679a773754e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading spacy tokenizers...\n",
            "creating dataset and iterator... \n",
            "creating dataset and iterator... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_pad = SRC.vocab.stoi['<pad>']\n",
        "trg_pad = TRG.vocab.stoi['<pad>']"
      ],
      "metadata": {
        "id": "UpiN3Btkx-N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "PATH_TO_CHECKPOINT = '/content/drive/My Drive/checkpoint_epoch_0.pth'\n",
        "\n",
        "checkpoint = torch.load(PATH_TO_CHECKPOINT, map_location=opt['device'])\n",
        "\n",
        "# Assuming the key for the model's parameters is 'model_state_dict'\n",
        "model_state_dict = checkpoint['model_state']"
      ],
      "metadata": {
        "id": "oWAZLti9YcXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt['d_model'], opt['n_layers'], opt['heads'], opt['dropout'])\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "model = model.to(opt['device'])"
      ],
      "metadata": {
        "id": "az666Btwx_j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = ScheduledOptim(\n",
        "        torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "        0.2, opt['d_model'], 4000)\n",
        "\n",
        "criterion = LabelSmoothingLoss(len(TRG.vocab), padding_idx=trg_pad, smoothing=0.1)"
      ],
      "metadata": {
        "id": "nSZFPIELyAEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(model_state_dict)\n",
        "start_epoch = checkpoint['epoch']\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state'])\n"
      ],
      "metadata": {
        "id": "kz_v7jyfY2A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "\n",
        "# step_num = 0\n",
        "# best_bleu = 0\n",
        "\n",
        "# for epoch in range(opt['epochs']):\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for i, batch in enumerate(train_iter):\n",
        "#         s = time.time()\n",
        "#         loss = step(model, optimizer, batch, criterion)\n",
        "\n",
        "#         total_loss += loss\n",
        "\n",
        "#         if (i + 1) % opt['printevery'] == 0:\n",
        "#             avg_loss = total_loss/opt['printevery']\n",
        "#             print('epoch: {:03d} - iter: {:05d} - train loss: {:.4f} - time: {:.4f}'.format(epoch, i, avg_loss, time.time()- s))\n",
        "#             total_loss = 0\n",
        "\n",
        "#     s = time.time()\n",
        "#     valid_loss, perplexity = validate(model, valid_iter, criterion)\n",
        "#     bleuscore = bleu(valid_src_data[:500], valid_trg_data[:500], model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "#     print('epoch: {:03d} - iter: {:05d} - valid loss: {:.4f} - bleu score: {:.4f} - perplexity: {:.4f} - time: {:.4f}'.format(epoch, i, valid_loss, bleuscore, perplexity, time.time() - s))\n"
      ],
      "metadata": {
        "id": "2e8jAKF7yBcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "step_num = 0\n",
        "best_bleu = 0\n",
        "\n",
        "for epoch in range(start_epoch, opt['epochs']):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        s = time.time()\n",
        "\n",
        "        # step() now returns the loss only; step_num is tracked here\n",
        "        info = step(model, optimizer, batch, criterion, step_num)\n",
        "        loss = info['loss']\n",
        "        step_num = info['step']\n",
        "\n",
        "        step_num += 1\n",
        "        total_loss += loss\n",
        "\n",
        "        if (i + 1) % opt['printevery'] == 0:\n",
        "            avg_loss = total_loss / opt['printevery']\n",
        "            print('epoch: {:03d} - iter: {:05d} - train loss: {:.4f} - time: {:.4f}'.format(\n",
        "                epoch, i, avg_loss, time.time() - s\n",
        "            ))\n",
        "            total_loss = 0\n",
        "\n",
        "    # Validation\n",
        "    s = time.time()\n",
        "    valid_loss, perplexity = validate(model, valid_iter, criterion)\n",
        "    bleuscore = bleu_sacre(\n",
        "        valid_src_data[:1000], valid_trg_data[:1000],\n",
        "        model, SRC, TRG,\n",
        "        opt['device'], opt['k'], opt['max_strlen']\n",
        "    )\n",
        "\n",
        "    print('epoch: {:03d} - iter: {:05d} - valid loss: {:.4f} - bleu score: {:.4f} - perplexity: {:.4f} - time: {:.4f}'.format(\n",
        "        epoch, i, valid_loss, bleuscore, perplexity, time.time() - s\n",
        "    ))\n",
        "\n",
        "    # track best BLEU\n",
        "    if bleuscore > best_bleu:\n",
        "        best_bleu = bleuscore\n",
        "\n",
        "    # SAVE CHECKPOINT EVERY EPOCH\n",
        "    save_checkpoint(\n",
        "        path=f\"checkpoint_epoch_{epoch}.pth\",\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        epoch=epoch,\n",
        "        step=step_num,\n",
        "        best_bleu=best_bleu\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MEYU4JZ7y_se",
        "outputId": "40948ba0-13d6-4d06-ac6f-679c68b30eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 000 - iter: 00199 - train loss: 3.0055 - time: 0.2690\n",
            "epoch: 000 - iter: 00399 - train loss: 3.0018 - time: 0.2938\n",
            "epoch: 000 - iter: 00599 - train loss: 2.9819 - time: 0.2834\n",
            "epoch: 000 - iter: 00799 - train loss: 3.0165 - time: 0.2792\n",
            "epoch: 000 - iter: 00999 - train loss: 3.0213 - time: 0.2903\n",
            "epoch: 000 - iter: 01199 - train loss: 2.9532 - time: 0.2780\n",
            "epoch: 000 - iter: 01399 - train loss: 2.9994 - time: 0.2982\n",
            "epoch: 000 - iter: 01599 - train loss: 2.9641 - time: 0.2805\n",
            "epoch: 000 - iter: 01799 - train loss: 2.9558 - time: 0.2915\n",
            "epoch: 000 - iter: 01999 - train loss: 3.0096 - time: 0.2916\n",
            "epoch: 000 - iter: 02199 - train loss: 3.0059 - time: 0.2777\n",
            "epoch: 000 - iter: 02399 - train loss: 3.0091 - time: 0.2827\n",
            "epoch: 000 - iter: 02599 - train loss: 2.9889 - time: 0.2952\n",
            "epoch: 000 - iter: 02799 - train loss: 3.0010 - time: 0.2837\n",
            "epoch: 000 - iter: 02999 - train loss: 2.9224 - time: 0.2789\n",
            "epoch: 000 - iter: 03199 - train loss: 3.0002 - time: 0.2960\n",
            "epoch: 000 - iter: 03399 - train loss: 2.9783 - time: 0.2924\n",
            "epoch: 000 - iter: 03599 - train loss: 2.9708 - time: 0.2950\n",
            "epoch: 000 - iter: 03799 - train loss: 2.9744 - time: 0.2934\n",
            "epoch: 000 - iter: 03999 - train loss: 2.9904 - time: 0.2719\n",
            "epoch: 000 - iter: 04199 - train loss: 2.9828 - time: 0.2937\n",
            "epoch: 000 - iter: 04399 - train loss: 2.9645 - time: 0.3101\n",
            "epoch: 000 - iter: 04599 - train loss: 2.9777 - time: 0.2937\n",
            "epoch: 000 - iter: 04799 - train loss: 2.9650 - time: 0.2944\n",
            "epoch: 000 - iter: 04999 - train loss: 2.9757 - time: 0.3007\n",
            "epoch: 000 - iter: 05199 - train loss: 2.9288 - time: 0.2862\n",
            "epoch: 000 - iter: 05399 - train loss: 2.9474 - time: 0.2814\n",
            "epoch: 000 - iter: 05599 - train loss: 2.9362 - time: 0.2926\n",
            "epoch: 000 - iter: 05799 - train loss: 2.9573 - time: 0.2786\n",
            "epoch: 000 - iter: 05999 - train loss: 2.9526 - time: 0.2743\n",
            "epoch: 000 - iter: 06199 - train loss: 2.9559 - time: 0.2800\n",
            "epoch: 000 - iter: 06399 - train loss: 2.9111 - time: 0.2816\n",
            "epoch: 000 - iter: 06599 - train loss: 2.9129 - time: 0.2892\n",
            "epoch: 000 - iter: 06799 - train loss: 2.9369 - time: 0.2953\n",
            "epoch: 000 - iter: 06999 - train loss: 2.9444 - time: 0.2869\n",
            "epoch: 000 - iter: 07199 - train loss: 2.9558 - time: 0.2758\n",
            "epoch: 000 - iter: 07399 - train loss: 2.9326 - time: 0.2837\n",
            "epoch: 000 - iter: 07599 - train loss: 2.9142 - time: 0.2929\n",
            "epoch: 000 - iter: 07799 - train loss: 2.9171 - time: 0.2926\n",
            "epoch: 000 - iter: 07999 - train loss: 2.9419 - time: 0.2846\n",
            "epoch: 000 - iter: 08199 - train loss: 2.8967 - time: 0.2825\n",
            "epoch: 000 - iter: 08399 - train loss: 2.9202 - time: 0.2898\n",
            "epoch: 000 - iter: 08599 - train loss: 2.8889 - time: 0.2618\n",
            "epoch: 000 - iter: 08799 - train loss: 2.9219 - time: 0.2779\n",
            "epoch: 000 - iter: 08999 - train loss: 2.9289 - time: 0.2903\n",
            "epoch: 000 - iter: 09199 - train loss: 2.9151 - time: 0.2767\n",
            "epoch: 000 - iter: 09399 - train loss: 2.8996 - time: 0.2879\n",
            "epoch: 000 - iter: 09599 - train loss: 2.9154 - time: 0.2962\n",
            "epoch: 000 - iter: 09799 - train loss: 2.9215 - time: 0.2869\n",
            "epoch: 000 - iter: 09999 - train loss: 2.8878 - time: 0.2883\n",
            "epoch: 000 - iter: 10199 - train loss: 2.9094 - time: 0.2947\n",
            "epoch: 000 - iter: 10399 - train loss: 2.9164 - time: 0.2920\n",
            "epoch: 000 - iter: 10599 - train loss: 2.8869 - time: 0.3138\n",
            "epoch: 000 - iter: 10799 - train loss: 2.8325 - time: 0.2972\n",
            "epoch: 000 - iter: 10999 - train loss: 2.7610 - time: 0.2028\n",
            "epoch: 000 - iter: 11199 - train loss: 2.7474 - time: 0.2845\n",
            "epoch: 000 - iter: 11399 - train loss: 2.6973 - time: 0.2914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 000 - iter: 11449 - valid loss: 2.6432 - bleu score: 41.2783 - perplexity: 14.0579 - time: 585.9241\n",
            "Checkpoint saved to checkpoint_epoch_0.pth\n",
            "epoch: 001 - iter: 00199 - train loss: 2.8459 - time: 0.3264\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1416816500.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# step() now returns the loss only; step_num is tracked here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstep_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2322500250.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(model, optimizer, batch, criterion, step_num)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_and_update_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3441436151.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtrue_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = bleu_sacre(valid_src_data[:1000], valid_trg_data[:1000], model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "print(score)"
      ],
      "metadata": {
        "id": "sdK4KiYjzo04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cccb4f79-620d-4b18-dca2-184c4d241c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41.249768470556994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing translation\n",
        "sentence='Following hemodynamic stabilization, the patient underwent contrast-enhanced computed tomography revealing multifocal ischemic lesions, prompting initiation of dual antiplatelet therapy and prophylactic low-molecular-weight heparin to mitigate thromboembolic risk.'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "H-KJy9I9zwWv",
        "outputId": "a5b911fe-d07b-4fe1-e790-28fc55f4e827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sau khi ổn định huyết động, bệnh nhân được chụp cắt lớp vi tính có tiêm thuốc cản quang cho thấy tổn thương thiếu máu cục bộ đa ổ, làm cho việc bắt đầu điều trị bằng thuốc kháng tiểu cầu kép và heparin trọng lượng phân tử thấp để giảm nguy cơ thuyên tắc huyết khối.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/checkpoint_epoch_0.pth /content/drive/MyDrive/checkpoint_epoch_0.pth\n"
      ],
      "metadata": {
        "id": "S0RL430xTFkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnOXeGrp1XEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}